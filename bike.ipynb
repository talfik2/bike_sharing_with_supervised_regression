{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92ce815",
   "metadata": {},
   "source": [
    "    In this repo, I applied Decision Tree Regressor, Random Forest, Gredient Boosting, and Ensemble Learning algorithms to their capabilities for predicting total number of bike rentals from the bike sharing systems dataset.\n",
    "    \n",
    "    Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n",
    "    I evaluated these model's by Mean Squared Error & Root Mean Squared Error.\n",
    "    \n",
    "    I also detected best hyperparameters for these models by Hyperparameter Tuning Method.\n",
    "    \n",
    "    Source of the dataset is: https://www.kaggle.com/c/bike-sharing-demand/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bc678",
   "metadata": {},
   "source": [
    "#### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c96bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e62823",
   "metadata": {},
   "source": [
    "#### GETTING KNOW OF OUR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5795e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df is:  (10886, 12)\n",
      "Does this data contain NA values? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike = pd.read_csv(\"C:\\\\Users\\\\talfi\\\\python\\\\ML\\\\datasets\\\\bike\\\\train.csv\")\n",
    "print(\"Shape of df is: \", bike.shape)\n",
    "print(\"Does this data contain NA values?\", bike.isnull().values.any())\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8c41e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime       object\n",
       "season          int64\n",
       "holiday         int64\n",
       "workingday      int64\n",
       "weather         int64\n",
       "temp          float64\n",
       "atemp         float64\n",
       "humidity        int64\n",
       "windspeed     float64\n",
       "casual          int64\n",
       "registered      int64\n",
       "count           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efe56ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season          int64\n",
       "holiday         int64\n",
       "workingday      int64\n",
       "weather         int64\n",
       "temp          float64\n",
       "atemp         float64\n",
       "humidity        int64\n",
       "windspeed     float64\n",
       "casual          int64\n",
       "registered      int64\n",
       "count           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime = bike.pop(\"datetime\")\n",
    "bike.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e526c2",
   "metadata": {},
   "source": [
    "#### SPLITTING DATA INTO TRAINING AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47b899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bike, bike[\"count\"],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c316a1",
   "metadata": {},
   "source": [
    "#### DECISION TREE  REGRESSOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ab042",
   "metadata": {},
   "source": [
    "Decision Tree Regressor is the first ML model I evaluated.  It uses a decision tree to go from observations about an item to conclusions about the item's target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19767311",
   "metadata": {},
   "source": [
    " <font color=blue> **HYPERPARAMATER TUNING**\n",
    "</font> <br>\n",
    "Let's tune hyperparamaters in a point that returns best performance.<br>\n",
    "Let's do this for Decision Tree Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1979a9a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best Hyperparameters for Decision Tree Regressor are:  {'max_depth': 8, 'min_samples_leaf': 1, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Let's make some hyperparameter tuning..\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"random_state\":[0,2,4,6,8],\n",
    "          \"max_depth\":[4,5,6,7,8],\n",
    "          \"min_samples_leaf\":[0.001, 0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(estimator = DecisionTreeRegressor(),\n",
    "                    param_grid = params,\n",
    "                    cv = 3,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                   verbose = 1, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_hyperparameters = grid.best_params_\n",
    "print(\"Best Hyperparameters for Decision Tree Regressor are: \", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd171f",
   "metadata": {},
   "source": [
    "Let's set the hyperparameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07ce1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "dt = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 1, random_state = 0)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47677f",
   "metadata": {},
   "source": [
    "####  MEAN SQUARED ERROR AND ROOT MEAN SQUARED ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef330c",
   "metadata": {},
   "source": [
    "**Mean Squared Error(MSE)** is an evaluation metric. The mean squared error tells you how close a regression line is to a set of points. It has values between 0 and infinity; lower  MSE is better concantration of the data around the line of best fit. <br>\n",
    "\n",
    "**Root Mean Squared Error(RMSE)**  is also an evaluation metric. It is the root of Mean Squared Error. It tells you how concentrated the data is around the line of best fit. It has values between 0 and infinity; lower  RMSE is better concantration of the data around the line of best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1309e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base MSE Score is:  5.236190247392217\n",
      "Base RMSE Score is:  2.288272328065918\n"
     ]
    }
   ],
   "source": [
    "mse_core = MSE(y_test, y_pred)\n",
    "print(\"Base MSE Score is: \", mse_core)\n",
    "rmse_core = mse_core **(1/2)\n",
    "print(\"Base RMSE Score is: \",rmse_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c6d1f",
   "metadata": {},
   "source": [
    "#### CROSS VALIDATION SCORE AND OVERFITTING / UNDERFITTING CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd4a0d",
   "metadata": {},
   "source": [
    "**Cross-validation** is a statistical method used to estimate the skill of machine learning models<br>\n",
    "\n",
    "**K-fold cross validation** is a procedure used to estimate the skill of the model on new data.<br>\n",
    "\n",
    "In this repo, we are going to use 10-Fold CV(K=10) to increase our Model's efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "339475d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "MSE_CV = -cross_val_score(dt, X_train, y_train,#output of CV Score'll be negative, by adding (-), I'll make it positive\n",
    "                         cv=10, scoring = \"neg_mean_squared_error\",#CV Scores doesn't allow to cumpute MSE directly\n",
    "                         n_jobs=-1)#to direct all CPU's for this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7649b",
   "metadata": {},
   "source": [
    "**Overfitting** is a modeling error that occurs when a function is too closely fit to a limited set of data points. A model overfits the data when it performs successfully on the training data and weakly on the test data.<br>\n",
    "\n",
    "**Underfitting** occurs when a model performs weekly not only on the training data but also in the test data. <br>\n",
    "\n",
    " In our case, \n",
    " * If CV MSE is bigger than training set MSE, our model overfits the data.\n",
    " * If CV MSE is roughly equal to the training set MSE, but much more bigger than the Base MSE, our model underfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07b37ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.99\n",
      "Training Set MSE:  0.6068964790430603\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train, y_train)\n",
    "# Predict the labels of the training set\n",
    "y_predict_train = dt.predict(X_train)\n",
    "# Predict the labels of test set\n",
    "y_predict_test = dt.predict(X_test)\n",
    "# CV MSE\n",
    "print(\"CV MSE: {:.2f}\".format(MSE_CV.mean()))\n",
    "# Training set MSE\n",
    "print(\"Training Set MSE: \",MSE(y_train, y_predict_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db55af",
   "metadata": {},
   "source": [
    "* As CV MSE is roughly equal to the Training Set MSE(just with a 0.30 difference), we can not diagnose the overfitting.\n",
    "* Moreover, because CV MSE & Training Set MSE smaller than Base MSE(2.28), we also can not diagnose the underfitting.\n",
    "* Hence, everything looks just fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb3705",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST REGRESSOR\n",
    " **Random Forest Regressor** takes the average of many decision trees. Each tree is weaker than a full decision tree, but by combining them, we get better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdf122",
   "metadata": {},
   "source": [
    " <font color=blue> **HYPERPARAMATER TUNING**\n",
    "</font> <br>\n",
    "Let's tune hyperparamaters in a point that returns best performance.<br>\n",
    "Let's do this for Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f5ae9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n",
      "Best Hyperparameters for Random Forest Regressor are:  {'max_depth': 8, 'n_estimators': 400, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Let's make some hyperparameter tuning..\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"random_state\":[0,2,4,6,8],\n",
    "          \"max_depth\":[4,5,6,7,8],\n",
    "          \"n_estimators\":[100,200,300,400,500]}\n",
    "grid = GridSearchCV(estimator = RandomForestRegressor(),\n",
    "                    param_grid = params,\n",
    "                    cv = 3,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                   verbose = 1, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_hyperparameters = grid.best_params_\n",
    "print(\"Best Hyperparameters for Random Forest Regressor are: \", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb05ab7",
   "metadata": {},
   "source": [
    "Let's set them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e9023ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Random Forest is:  1.3026320970331224\n",
      "MSE of Random Forest is:  1.69685038022091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 400, max_depth = 8, random_state = 0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print(\"RMSE of Random Forest is: \",rmse_test)\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "print(\"MSE of Random Forest is: \",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814b4a5",
   "metadata": {},
   "source": [
    "See, both scores are less than Decision Tree Regressor's scores. <br>\n",
    "This means that Random Forest Regressor more successful to concentrated the data around the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1607f5fa",
   "metadata": {},
   "source": [
    "#### GRADIENT BOOSTING REGRESSOR\n",
    "**Gradient Boosting Regressor** uses even weaker decision trees that increasingly focused on hard examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24ab9f",
   "metadata": {},
   "source": [
    " <font color=blue> **HYPERPARAMATER TUNING**\n",
    "</font> <br>\n",
    "Let's tune hyperparamaters in a point that returns best performance.<br>\n",
    "Let's do this for Gradient Boosting Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3c3d6bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n",
      "Best Hyperparameters for Random Forest Regressor are:  {'max_depth': 6, 'n_estimators': 500, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Let's make some hyperparameter tuning..\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"random_state\":[0,2,4,6,8],\n",
    "          \"max_depth\":[4,5,6,7,8],\n",
    "          \"n_estimators\":[100,200,300,400,500]}\n",
    "grid = GridSearchCV(estimator = GradientBoostingRegressor(),\n",
    "                    param_grid = params,\n",
    "                    cv = 3,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                   verbose = 1, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_hyperparameters = grid.best_params_\n",
    "print(\"Best Hyperparameters for Random Forest Regressor are: \", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d674e2b",
   "metadata": {},
   "source": [
    "Let's set these hyperparameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1910a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Gradient Boosting is:  0.5949756651371644\n",
      "MSE of Gradient Boosting is:  0.35399604210541114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbt = GradientBoostingRegressor(n_estimators = 500, max_depth = 6, random_state = 0)\n",
    "gbt.fit(X_train, y_train)\n",
    "y_pred = gbt.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print(\"RMSE of Gradient Boosting is: \",rmse_test)\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "print(\"MSE of Gradient Boosting is: \",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de110a01",
   "metadata": {},
   "source": [
    "Both scores are less than Decision Tree and Random Forest Regressors' scores. <br>\n",
    "This means that Gradient Boosting Regressor more successful to concentrated the data around the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935c2b7",
   "metadata": {},
   "source": [
    "#### ENSEMBLE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c119d4",
   "metadata": {},
   "source": [
    "**Ensemble learning** is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. In ensemble learning,<br>\n",
    "* First, we train different models in the same dataset.\n",
    "* Then, train models make predictions.\n",
    "* We create a meta model by merging existing models.\n",
    "* Our meta model is the model of the ensemble learning. <br>\n",
    "In this repo, we are going to set our meta model as `VotingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a721fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Decision Tree Regressor : 2.288\n",
      "RMSE for Random Forest Regressor : 1.303\n",
      "RMSE For Gradient Boosting Regressor : 0.595\n",
      "RMSE for Voting Regressor : 1.3264669493804975\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 400, max_depth = 8, random_state = 0)\n",
    "dt = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 1, random_state = 0)\n",
    "gbt = GradientBoostingRegressor(n_estimators = 500, max_depth = 6, random_state = 0)\n",
    "regressors = [(\"RMSE for Decision Tree Regressor\", dt), (\"RMSE for Random Forest Regressor\", rf), (\"RMSE For Gradient Boosting Regressor\", gbt)]\n",
    "# For Loop is for returning results of the algorithms above\n",
    "for rg_name, rg in regressors:\n",
    "    rg.fit(X_train, y_train)\n",
    "    y_pred = rg.predict(X_test)\n",
    "    print(\"{:s} : {:.3f}\".format(rg_name, (MSE(y_pred, y_test)**(1/2))))\n",
    "# Below is for Voting Regressor\n",
    "vc = VotingRegressor(estimators = regressors)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print(\"RMSE for Voting Regressor :\", (MSE(y_pred,y_test)**(1/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec3c64",
   "metadata": {},
   "source": [
    "Voting Regressor performed pretty well, but not as good as Gredient Boosting Regressor. This is what Ensemble Learning does, performs pretty well, but in some cases; it may not perform the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
